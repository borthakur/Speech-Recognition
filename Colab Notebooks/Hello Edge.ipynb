{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hello Edge.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"8249gaqhyL22"},"source":["Research Paper - https://arxiv.org/pdf/1711.07128.pdf - https://github.com/ARM-software/ML-KWS-for-MCU <br>\n","Github Repos - <br>\n","https://github.com/rcmalli/keras-mobilenet <br>\n","https://github.com/ZainNasrullah/music-artist-classification-crnn"]},{"cell_type":"code","metadata":{"id":"D7bcBn78yI4t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612805030002,"user_tz":-330,"elapsed":75094,"user":{"displayName":"Croam Speech","photoUrl":"","userId":"11800440030975940372"}},"outputId":"e7e71fa5-c468-43fe-bac7-9b6544026438"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)\n","path = '/content/gdrive/My Drive/'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7CKJZj5aqTO","executionInfo":{"status":"ok","timestamp":1612805031165,"user_tz":-330,"elapsed":76227,"user":{"displayName":"Croam Speech","photoUrl":"","userId":"11800440030975940372"}},"outputId":"329ac6f4-ff4e-4c61-dd74-5bf4c779d002"},"source":["pip install keras_applications==1.0.4 --no-deps\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting keras_applications==1.0.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/8f327deaa37a71caddb59b7b4aaa9d4b3e90c0e76f8c2d1572005278ddc5/Keras_Applications-1.0.4-py2.py3-none-any.whl (43kB)\n","\r\u001b[K     |███████▌                        | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 21.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 30kB 11.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n","\u001b[?25hInstalling collected packages: keras-applications\n","Successfully installed keras-applications-1.0.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lR4QZ8Qrq9BE","executionInfo":{"status":"ok","timestamp":1612805055730,"user_tz":-330,"elapsed":1538,"user":{"displayName":"Croam Speech","photoUrl":"","userId":"11800440030975940372"}},"outputId":"65a2486e-ec64-4a67-a470-6190991f4e30"},"source":["pip install depthwise_conv2d"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement depthwise_conv2d (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for depthwise_conv2d\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7jhdG-JpxJCz","colab":{"base_uri":"https://localhost:8080/","height":334},"executionInfo":{"status":"error","timestamp":1612801208110,"user_tz":-330,"elapsed":1030,"user":{"displayName":"Croam Speech","photoUrl":"","userId":"11800440030975940372"}},"outputId":"c411f7e5-89ed-46b7-9d95-3627461d7824"},"source":["import librosa\n","import tensorflow as tf\n","import keras\n","from keras.layers import Dense,LSTM,GRU,GlobalMaxPool1D,Bidirectional,MaxPooling2D\n","from keras.models import Sequential\n","import numpy as np\n","import os\n","from sklearn.utils import shuffle\n","from sklearn import metrics\n","import pickle\n","from scipy.io.wavfile import read,write\n","\n","#from keras_applications.imagenet_utils import _obtain_input_shape\n","from keras import backend as K\n","from keras.layers import Input, Convolution2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Dropout, Permute, Reshape\n","from keras.models import Model\n","from keras.engine.topology import get_source_inputs\n","\n","'''\n","os.chdir is throwing some error which I am not able to resolve. For the time being, do the following to import this python script -\n","Find the file in the drive and download it, then reupload it in the /content folder of the colab file.\n","You will need to do this for every new runtime connection\n","'''\n","from tensorflow.keras.layers import Conv2D\n","\n","from depthwise_conv2d import DepthwiseConvolution2D"],"execution_count":15,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-730ba9fc9014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdepthwise_conv2d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDepthwiseConvolution2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'depthwise_conv2d'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"_scaFGj-yqqW"},"source":["#LOAD AND PROCESS INPUT"]},{"cell_type":"code","metadata":{"id":"W25qnUPKx1ar"},"source":["# KEYWORD_FOLDER1= 'Bachao_Data_Old/'\n","# KEYWORD_FOLDER2= 'Bachao_Data_Babble_10dB'\n","# KEYWORD_FOLDER3 = 'Bachao_Data_Natural_10dB'\n","\n","KEYWORD_FOLDER4 = path + 'Help_Data_Old'\n","KEYWORD_FOLDER5 = path + 'Help_Data_10dB'\n","KEYWORD_FOLDER6= path + 'Help_Data_Natural_10dB'\n","\n","\n","# NEGATIVE_FOLDER1 = 'Negative_Data/'\n","# NEGATIVE_FOLDER2 = 'Negative_Data_10dB'\n","# NEGATIVE_FOLDER3 = 'Negative_Data_Natural_10dB'\n","NEGATIVE_FOLDER4 = path + 'Negative_Data/'\n","NEGATIVE_FOLDER5 = path + 'Negative_Data_10dB'\n","NEGATIVE_FOLDER6 = path + 'Negative_Data_Natural_10dB'\n","\n","#OPPPOSITE_KEYWORD_FOLDER = 'Bachao_Data/'\n","KEYWORD_FOLDER_TEST = path + 'Bachao_Data_Test/'\n","NEGATIVE_FOLDER_TEST = path + 'Negative_Data_Test_Old/'\n","#OPPPOSITE_KEYWORD_FOLDER_TEST = 'Bachao_Data_Test/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m7OkpCT2yaC4"},"source":["def count_files(folder, extension):\n","\tcount = 0\n","\tfor file in os.listdir(folder):\n","\t\tif file.endswith(extension):\n","\t\t\tfile_path = os.path.join(folder, file)\n","\t\t\tcount += 1\n","\treturn count\n","\n","def load_data_folder(folder, is_keyword):\n","  num_samples = count_files(folder, '.wav')\n","  data_X = np.zeros((num_samples, INPUT_SHAPE[0], INPUT_SHAPE[1]), dtype=np.float64)\n","  data_Y = np.zeros((num_samples), dtype=np.float64)\n","\n","  count = 0\n","  for file in os.listdir(folder):\n","    if file.endswith('.wav'):\n","      file_path = os.path.join(folder, file)\n","      y, sr = librosa.load(file_path,sr=None)\n","      mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=128, n_fft=256, n_mfcc=20)\n","      mfcc_delta = librosa.feature.delta(mfcc)[:10, :]\n","      mfcc_double_delta = librosa.feature.delta(mfcc, order=2)[:10, :]\n","      data_X[count, :, :20] = mfcc.T\n","      data_X[count, :, 20:30] = mfcc_delta.T\n","      data_X[count, :, 30:] = mfcc_double_delta.T\n","      data_Y[count] = int(is_keyword)\n","      count += 1\n","      if count%50==0:\n","        print(count)\n","  return data_X, data_Y\n","\n","def load_data(folders):\n","\tnum_samples = sum([count_files(folder, '.wav') for folder, is_keyword in folders])\n","\tdata_X = np.zeros((num_samples, INPUT_SHAPE[0], INPUT_SHAPE[1]), dtype=np.float64)\n","\tdata_Y = np.zeros((num_samples), dtype=np.float64)\n","\tcount = 0\n","\tfor folder, is_keyword in folders:\n","\t\tnum_samples_folder = count_files(folder, '.wav')\n","\t\tdata_X[count:count+num_samples_folder, :, :], data_Y[count:count+num_samples_folder] = (\n","\t\t\tload_data_folder(folder, is_keyword))\n","\t\tcount += num_samples_folder\n","\treturn shuffle(data_X, data_Y, random_state=0)\n","\n","def load_train_data():\n","  #folders = [(NEGATIVE_FOLDER_TRAIN_1, False), (NEGATIVE_FOLDER_TRAIN_2, False), (NEGATIVE_FOLDER_TRAIN_3, False)]\n","  folders = [(KEYWORD_FOLDER4, True), (NEGATIVE_FOLDER4, False)]\n","  #folders = [(KEYWORD_FOLDER1, True), (KEYWORD_FOLDER2, True), (KEYWORD_FOLDER3, True), (KEYWORD_FOLDER4, True), (KEYWORD_FOLDER5, True), (KEYWORD_FOLDER6, True), (KEYWORD_FOLDER_TEST, True), (NEGATIVE_FOLDER1, False), (NEGATIVE_FOLDER2, False), (NEGATIVE_FOLDER3, False), (NEGATIVE_FOLDER4, False), (NEGATIVE_FOLDER5, False), (NEGATIVE_FOLDER6, False), (NEGATIVE_FOLDER_TEST, False)]\n","  return load_data(folders)\n","\n","def load_test_data():\n","  #folders = [(NEGATIVE_FOLDER_TEST, True)]\n","  folders = [(KEYWORD_FOLDER_TEST, True), (NEGATIVE_FOLDER_TEST, False)] \n","  return load_data(folders)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UVW_Qvwew9ZE"},"source":["activ_dir = 'white_noise'\n","fs, x = read(path + 'white_noise.wav')\n","file_size = x.shape[0]\n","segment_time = 3.0\n","segment_samples = int(segment_time * fs)\n","no_of_segments = int(file_size/segment_samples)\n","\n","# for i in range(no_of_segments - 1):\n","#   file_name = '{}_{}.wav'.format(activ_dir, i)\n","#   x_temp = x[i*segment_samples:(i+1)*segment_samples]\n","#   write(path + 'white_noice/' + file_name, fs, x_temp)\n","\n","print(fs)\n","print(x.shape)\n","print(file_size)\n","print(segment_samples)\n","print(no_of_segments)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-YgqF6Tyogf"},"source":["with open(path + 'help_data_total_hari_train_x.pickle', 'rb') as f:\n","  train_X = pickle.load(f)\n","with open(path + 'help_data_total_hari_train_y.pickle', 'rb') as f:\n","  train_Y = pickle.load(f)\n","\n","print(\"Train data extracted\")\n","\n","print(train_Y.sum())\n","print(train_Y.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"azdZw-sUzeK_"},"source":["np.random.seed(0)\n","np.random.shuffle(train_X)\n","np.random.shuffle(train_Y)\n","\n","train_X_shuffle = train_X\n","train_Y_shuffle = train_Y\n","\n","train_X_shuffle = train_X_shuffle[:,:,:,np.newaxis]\n","\n","print(train_X_shuffle.shape)\n","print(train_Y_shuffle.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RzcNElPGzreC"},"source":["# import sklearn\n","# train_X_cv, test_X_cv, train_Y_cv, test_Y_cv = sklearn.model_selection.train_test_split(train_X_shuffle, train_Y_shuffle, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YR6FuKaDywOW"},"source":["#MODEL"]},{"cell_type":"code","metadata":{"id":"e86H5MrHyxfB"},"source":["def MobileNet(input_shape=(376,40,1), alpha=1, classes=2):\n","    \"\"\"Instantiates the MobileNet.Network has two hyper-parameters\n","        which are the width of network (controlled by alpha)\n","        and input size.\n","        \n","        # Arguments\n","            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n","                to use as image input for the model.\n","            input_shape: optional shape tuple, only to be specified\n","                if `include_top` is False (otherwise the input shape\n","                has to be `(224, 224, 3)` (with `channels_last` data format)\n","                or `(3, 224, 244)` (with `channels_first` data format).\n","                It should have exactly 3 inputs channels,\n","                and width and height should be no smaller than 96.\n","                E.g. `(200, 200, 3)` would be one valid value.\n","            alpha: optional parameter of the network to change the \n","                width of model.\n","            shallow: optional parameter for making network smaller.\n","            classes: optional number of classes to classify images\n","                into.\n","        # Returns\n","            A Keras model instance.\n","        \"\"\"\n","\n","    img_input = Input(shape=input_shape)\n","\n","    x = Convolution2D(int(512 * alpha), (3, 3), strides=(2, 2), padding='same', use_bias=False)(img_input)\n","    x = BatchNormalization()(x)\n","    x = Activation('elu')(x)\n","\n","    x = DepthwiseConvolution2D(int(512 * alpha), (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('elu')(x)\n","    x = Convolution2D(int(512 * alpha), (1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('elu')(x)\n","\n","    x = DepthwiseConvolution2D(int(512 * alpha), (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('elu')(x)\n","    x = Convolution2D(int(512 * alpha), (1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('elu')(x)\n","\n","    x = DepthwiseConvolution2D(int(512 * alpha), (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('elu')(x)\n","    x = Convolution2D(int(512 * alpha), (1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('elu')(x)\n","\n","    x = DepthwiseConvolution2D(int(512 * alpha), (3, 3), strides=(2, 2), padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('elu')(x)\n","    x = Convolution2D(int(512 * alpha), (1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('elu')(x)\n","\n","    x = DepthwiseConvolution2D(int(512 * alpha), (3, 3), strides=(1, 1), padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('elu')(x)\n","    x = Convolution2D(int(512 * alpha), (1, 1), strides=(1, 1), padding='same', use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('elu')(x)\n","\n","    x = GlobalAveragePooling2D()(x)\n","    out = Dense(classes, activation='softmax')(x)\n","\n","    model = Model(img_input, out, name='mobilenet')\n","\n","    return model\n","\n","def CRNN1(X_shape = (376,40,1), nb_classes = 2):\n","    '''\n","    Model used for evaluation in paper. Inspired by K. Choi model in:\n","    https://github.com/keunwoochoi/music-auto_tagging-keras/blob/master/music_tagger_crnn.py\n","    '''\n","\n","    nb_layers = 4  # number of convolutional layers\n","    nb_filters = [256, 512, 512, 512]  # filter sizes\n","    kernel_size = (3, 3)  # convolution kernel size\n","    activation = 'elu'  # activation function to use after each layer\n","    pool_size = [(2, 2), (4, 2), (4, 2), (4, 2),\n","                 (4, 2)]  # size of pooling area\n","\n","    # shape of input data (frequency, time, channels)\n","    input_shape = (X_shape[0], X_shape[1], X_shape[2])\n","    frequency_axis = 1\n","    time_axis = 2\n","    channel_axis = 3\n","\n","    # Create sequential model and normalize along frequency axis\n","    model = Sequential()\n","    #model.add(BatchNormalization(axis=frequency_axis, input_shape=input_shape))\n","\n","    # First convolution layer specifies shape\n","    model.add(Convolution2D(nb_filters[0], kernel_size=kernel_size, padding='same',\n","                     data_format=\"channels_last\",\n","                     input_shape=input_shape))\n","    model.add(Activation(activation))\n","    model.add(BatchNormalization(axis=channel_axis))\n","    model.add(MaxPooling2D(pool_size=pool_size[0], strides=pool_size[0]))\n","    model.add(Dropout(0.1))\n","\n","    # Add more convolutional layers\n","    for layer in range(nb_layers - 1):\n","        # Convolutional layer\n","        model.add(Convolution2D(nb_filters[layer + 1], kernel_size=kernel_size,\n","                         padding='same'))\n","        model.add(Activation(activation))\n","        model.add(BatchNormalization(\n","            axis=channel_axis))  # Improves overfitting/underfitting\n","        model.add(MaxPooling2D(pool_size=pool_size[layer + 1],\n","                               strides=pool_size[layer + 1]))  # Max pooling\n","        model.add(Dropout(0.1))\n","\n","        # Reshaping input for recurrent layer\n","    # (frequency, time, channels) --> (time, frequency, channel)\n","    model.add(Permute((time_axis, frequency_axis, channel_axis)))\n","    resize_shape = model.output_shape[2] * model.output_shape[3]\n","    model.add(Reshape((model.output_shape[1], resize_shape)))\n","\n","    # recurrent layer\n","    model.add(Bidirectional(GRU(256, return_sequences=True)))\n","    model.add(Bidirectional(GRU(256, return_sequences=False)))\n","    model.add(Dropout(0.3))\n","\n","    # Output layer\n","    model.add(Dense(nb_classes))\n","    model.add(Activation(\"softmax\"))\n","    return model\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLJ1gjbdtFFT"},"source":["model = CRNN1()\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zK2hgu3fENgP"},"source":["from keras.optimizers import Adam\n","model.compile(optimizer = Adam(learning_rate = 0.001),loss = tf.keras.losses.SparseCategoricalCrossentropy(),metrics = ['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b7JMmm63g3NW"},"source":["model.fit(x=train_X_shuffle,y=train_Y_shuffle,batch_size=64,epochs=10,validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A1QHCEsGhhfG"},"source":[""],"execution_count":null,"outputs":[]}]}